
"""
Gradio ‚Äì Dictaphone/Uploader ‚Üí Transcription (FFmpeg v8 Whisper) ‚Üí R√©sum√© CR via LM Studio (Gradio 4.x)

Correctifs qualit√©
------------------
- **D√©but manquant / silences** : sur certains fichiers, les premiers mots peuvent √™tre mang√©s si le mod√®le d√©marre trop vite ou si le VAD est trop agressif.
  - Ajout d‚Äôun **pr√©‚Äëroll** (padding de silence en t√™te) via `adelay=‚Ä¶` (par d√©faut 250 ms) pour capturer les d√©buts de phrase.
  - **VAD (facultatif)** : possibilit√© d‚Äôactiver un mod√®le VAD Silero et d‚Äôajuster seuil/durations. Par d√©faut **d√©sactiv√©** pour ne pas couper les blancs.
  - Choix du **format de sortie** `text | srt | json` (par d√©faut `srt` pour v√©rifier visuellement les segments + horodatages).

R√©f√©rences
----------
- Options officielles du filtre `whisper` (mod√®le, destination, format, VAD : `vad_model`, `vad_threshold`, `vad_min_*`, `queue`). ÓàÄciteÓàÇturn2view0ÓàÅ
- Pr√©sentation des options `destination`, `format` et VAD dans des exemples r√©cents. ÓàÄciteÓàÇturn1search11ÓàÇturn0search0ÓàÅ
- `adelay=‚Ä¶:all=1` (ajout de silence en t√™te sur tous les canaux). ÓàÄciteÓàÇturn3search16ÓàÅ

Pr√©requis
---------
- Python 3.10+
- `pip install gradio requests python-dotenv`
- FFmpeg 8.0+ **compil√© avec `--enable-whisper`** + `whisper.cpp` pr√©sent
- LM Studio en mode serveur local (OpenAI-compatible) ‚Äì http://localhost:1234

Lancement
---------
python gradio_transcripteur_compte_rendu.py

"""

import os
import json
import subprocess
from datetime import datetime
from typing import Tuple

import gradio as gr
import requests
from dotenv import load_dotenv

# ----------------------
# Config (.env facultatif)
# ----------------------
load_dotenv()
LMSTUDIO_BASE_URL = os.getenv("LMSTUDIO_BASE_URL", "http://localhost:1234")
LMSTUDIO_API_PATH = os.getenv("LMSTUDIO_API_PATH", "/v1/chat/completions")
LMSTUDIO_MODEL = os.getenv("LMSTUDIO_MODEL", "Qwen2.5-7B-Instruct")
LMSTUDIO_API_KEY = os.getenv("LMSTUDIO_API_KEY", "lm-studio")

FFMPEG_BIN = os.getenv("FFMPEG_BIN", "ffmpeg")
WHISPER_MODEL_PATH = os.getenv("WHISPER_MODEL_PATH", "./models/ggml-large-v3-turbo.bin")
WHISPER_LANGUAGE = os.getenv("WHISPER_LANGUAGE", "fr")

os.makedirs("transcripts", exist_ok=True)

# ----------------------
# Utilitaires
# ----------------------

def ffmpeg_has_whisper() -> bool:
    try:
        proc = subprocess.run([FFMPEG_BIN, "-hide_banner", "-filters"], capture_output=True, text=True, check=True)
        return "whisper" in proc.stdout
    except Exception:
        return False


def make_out_rel(suffix: str = "txt") -> str:
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    rel = f"transcripts/transcript_{stamp}.{suffix}"
    return rel.replace("\\", "/")


def sanitize_path(p: str) -> str:
    return os.path.normpath(p).replace("\\", "/")


def run_ffmpeg_whisper_transcribe(
    audio_path: str,
    language: str = WHISPER_LANGUAGE,
    fmt: str = "srt",
    preroll_ms: int = 250,
    vad_enabled: bool = False,
    vad_model_path: str = "",
    vad_threshold: float = 0.5,
    vad_min_speech_ms: int = 100,
    vad_min_silence_ms: int = 500,
    queue_ms: int = 3000,
) -> Tuple[str, str]:
    """Transcrit `audio_path` via FFmpeg+Whisper.
    Retourne (texte_ou_sous-titres, chemin_fichier_sortie_relatif).
    """
    if not os.path.isfile(audio_path):
        raise FileNotFoundError(f"Fichier introuvable: {audio_path}")

    if not ffmpeg_has_whisper():
        raise RuntimeError("FFmpeg ne comporte pas le filtre 'whisper'. Build FFmpeg 8 avec --enable-whisper requis.")

    # Fichier de sortie selon format demand√©
    ext = "txt" if fmt == "text" else ("srt" if fmt == "srt" else "json")
    out_rel = make_out_rel(ext)

    # Options whisper
    whisper_opts = [
        f"model={sanitize_path(WHISPER_MODEL_PATH)}",
        f"language={language}",
        f"destination={out_rel}",
        f"format={fmt}",
        f"queue={queue_ms}ms",
    ]

    # VAD facultatif (Silero). √Ä activer uniquement si un mod√®le est fourni.
    if vad_enabled and vad_model_path:
        whisper_opts.append(f"vad_model={sanitize_path(vad_model_path)}")
        whisper_opts.append(f"vad_threshold={vad_threshold}")
        whisper_opts.append(f"vad_min_speech_duration={max(20, int(vad_min_speech_ms))}ms")
        whisper_opts.append(f"vad_min_silence_duration={max(0, int(vad_min_silence_ms))}ms")

    whisper_filter = "whisper=" + ":".join(whisper_opts)

    # Cha√Æne audio : pr√©‚Äëroll (adelay) pour capturer le tout d√©but + resample pour stabilit√©
    af_chain = []
    if preroll_ms and preroll_ms > 0:
        af_chain.append(f"adelay={int(preroll_ms)}:all=1")
    af_chain.append("aresample=16000")
    af_chain.append(whisper_filter)

    af_str = ",".join(af_chain)

    cmd = [
        FFMPEG_BIN,
        "-y",
        "-hide_banner",
        "-i",
        audio_path,
        "-vn",
        "-af",
        af_str,
        "-f",
        "null",
        "-",
    ]

    proc = subprocess.run(cmd, capture_output=True, text=True)
    if proc.returncode != 0:
        raise RuntimeError(
            f"""√âchec FFmpeg Whisper ({proc.returncode})
    STDERR:
    {proc.stderr}
    CMD:
    {' '.join(cmd)}"""
        )

    if not os.path.exists(out_rel):
        raise FileNotFoundError(
            f"""Sortie non trouv√©e: {out_rel}
    V√©rifiez mod√®le/chemins et options du filtre."""
        )


    with open(out_rel, "r", encoding="utf-8") as f:
        text = f.read().strip()

    return text, out_rel


def call_lmstudio_summary(transcript: str) -> str:
    url = LMSTUDIO_BASE_URL.rstrip("/") + LMSTUDIO_API_PATH
    system_prompt = (
        "Tu es un assistant sp√©cialis√© en comptes rendus de r√©union. "
        "√Ä partir de la transcription (peut contenir des horodatages), g√©n√®re un CR clair en fran√ßais (Markdown) : Contexte, Ordre du jour, D√©cisions, Actions (Responsable‚ÜíAction‚Üí√âch√©ance), Prochaines √©tapes, Citations, Risques."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": transcript},
    ]

    payload = {"model": LMSTUDIO_MODEL, "messages": messages, "temperature": 0.2, "max_tokens": 1800}
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {LMSTUDIO_API_KEY}"}

    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)
    resp.raise_for_status()
    data = resp.json()
    return data["choices"][0]["message"]["content"].strip()

# ----------------------
# Callbacks Gradio
# ----------------------

def do_transcribe(audio_path: str, language: str, fmt: str, preroll_ms: int, vad_enabled: bool, vad_model_path: str, vad_threshold: float, vad_min_speech_ms: int, vad_min_silence_ms: int, queue_ms: int):
    if not audio_path:
        return "Aucun fichier fourni.", "", ""
    text, out_rel = run_ffmpeg_whisper_transcribe(
        audio_path=audio_path,
        language=language or WHISPER_LANGUAGE,
        fmt=fmt,
        preroll_ms=preroll_ms,
        vad_enabled=vad_enabled,
        vad_model_path=vad_model_path,
        vad_threshold=vad_threshold,
        vad_min_speech_ms=vad_min_speech_ms,
        vad_min_silence_ms=vad_min_silence_ms,
        queue_ms=queue_ms,
    )
    return f"Transcription OK ({len(text)} caract√®res)", text, out_rel


def do_summarize(transcript: str):
    if not transcript or not transcript.strip():
        return "Pas de transcription √† r√©sumer.", ""
    md = call_lmstudio_summary(transcript)
    stamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    return f"R√©sum√© g√©n√©r√© ({len(md)} caract√®res) ‚Äì {stamp}", md

# ----------------------
# UI (Gradio 4.x)
# ----------------------
with gr.Blocks(title="Transcripteur ‚Üí CR R√©union (FFmpeg Whisper)") as demo:
    gr.Markdown(
        """
        # üéôÔ∏è Transcripteur ‚Üí CR de r√©union
        1) **Enregistrer / Uploader** un audio (WAV/MP3/M4A).  
        2) **Transcrire** (FFmpeg v8 + Whisper) ‚Äì format par d√©faut **SRT** pour voir les horodatages.  
        3) **R√©sumer** (LM Studio) ‚Üí **CR Markdown**.
        
        *Astuce* : si le **d√©but est tronqu√©**, augmentez le **pr√©‚Äëroll** (ex. 300‚Äì500 ms) ou laissez le **VAD d√©sactiv√©**.
        """
    )

    with gr.Row():
        audio = gr.Audio(type="filepath", label="üéôÔ∏è Enregistrer ou Uploader")
        lang = gr.Textbox(label="Langue Whisper", value=WHISPER_LANGUAGE)
        fmt = gr.Dropdown(choices=["text", "srt", "json"], value="srt", label="Format sortie")

    with gr.Accordion("Param√®tres avanc√©s", open=False):
        preroll = gr.Slider(0, 2000, value=250, step=50, label="Pr√©‚Äëroll (ms) ‚Äì padding d√©but")
        queue = gr.Slider(200, 20000, value=3000, step=100, label="Taille de file (queue) pour VAD (ms)")
        vad_enable = gr.Checkbox(False, label="Activer VAD (Silero) ‚Äì peut couper les blancs si mal r√©gl√©")
        with gr.Row():
            vad_model = gr.Textbox(label="Chemin mod√®le VAD (ex: ./models/silero-v5.1.2-ggml.bin)", value="")
            vad_thr = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="Seuil VAD")
        with gr.Row():
            vad_min_speech = gr.Slider(20, 2000, value=100, step=20, label="Dur√©e minimale parole (ms)")
            vad_min_silence = gr.Slider(0, 2000, value=500, step=20, label="Dur√©e minimale silence (ms)")

    btn_transcribe = gr.Button("üìù Transcrire")
    status_trans = gr.Textbox(label="Statut transcription", interactive=False)
    transcript = gr.Textbox(label="Transcription / SRT / JSON", lines=16)
    transcript_file = gr.Textbox(label="Fichier g√©n√©r√©", interactive=False)

    gr.Markdown("---")

    btn_summarize = gr.Button("üßæ R√©sumer ‚Üí CR Markdown")
    status_sum = gr.Textbox(label="Statut r√©sum√©", interactive=False)
    summary_md = gr.Markdown("(le CR appara√Ætra ici)")

    # Wiring
    btn_transcribe.click(
        fn=do_transcribe,
        inputs=[audio, lang, fmt, preroll, vad_enable, vad_model, vad_thr, vad_min_speech, vad_min_silence, queue],
        outputs=[status_trans, transcript, transcript_file],
    )
    btn_summarize.click(fn=do_summarize, inputs=[transcript], outputs=[status_sum, summary_md])

if __name__ == "__main__":
    demo.launch()
